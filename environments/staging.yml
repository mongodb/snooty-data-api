service:
  targetPort: 3000

env:
  PORT: 3000
  SNOOTY_DB_NAME: snooty_stage
  SNOOTY_PROD_DB_NAME: snooty_dotcomstg
  POOL_DB_NAME: pool_test
  SNOOTY_ENV: dotcomstg

envSecrets:
  ATLAS_URI: snooty-data-api-staging

ingress:
  enabled: true
  hosts:
    - snooty-data-api.docs.staging.corp.mongodb.com

# Current limits and requests are based around metrics during expected peaks
resources:
  limits:
    memory: 3Gi
    cpu: 2
  requests:
    memory: 1100Mi
    cpu: 400m

# The current probes are pretty lenient to avoid prematurely shutting down
# pods during mass fetches. They should be adjusted accordingly in the future
probes:
  enabled: true
  path: /liveness
  headers: {}
  # Determines if the pod is still alive. The pod will shut down if this fails.
  liveness:
    httpGet: true
    initialDelaySeconds: 10
    periodSeconds: 30
    # Ideally, timeout would be lower, but this prevents any latency during memory spikes
    timeoutSeconds: 10
    successThreshold: 1
    # Keeping failureThreshold for liveness higher than readiness to ensure
    # pod stays up even if it's not ready
    failureThreshold: 6
  # Determines when the pod is ready to receive traffic. This may help with ensuring that requests
  # don't disappear into the void when routed to a new pod that isn't ready yet
  readiness:
    httpGet: true
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 10
    successThreshold: 1
    failureThreshold: 3

autoscaling:
  apiVersion: autoscaling/v2
  minReplicas: 3
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 75
        name: memory
        target:
          type: Utilization
          averageUtilization: 75
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0 # scaleUp immediately if conditions met
      policies:
        - type: Pods
          value: 1
          periodSeconds: 60
    scaleDown:
     stabilizationWindowSeconds: 60
     policies:
      # Take down 1 pod every 60 seconds, assuming usage is stable
       - type: Pods
         value: 1
         periodSeconds: 60
